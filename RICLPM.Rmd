---
title: "CLPM scripts"
author: "Toby"
date: "26 november 2022"
output: pdf_document
---

# Random Intercept Cross Lagged Panel Model

## Introduction

This Markdown file contains all relevant code and scripts to run a RI-CLPM, starting from a long-format raw data file. First we will transform that long format data into wide format data, where the measurements at later time points will be in the columns of the dataset. After than, we will run our tests of measurement invariance, starting with the comparison of weak measurment invariance to the configural model, followed by strong measurement invariance and finally using more advanced RI structures.


First, we need to load relevant packages and set the working directory to the folder with all data and scripts. We also set a seed that makes all random elements of the script reproducible.

```{r, warning = FALSE}
require(readxl)
require(lavaan)
require(psych)
require(tinytex)

setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/6. R scripts/CLPM model selection")

set.seed(1234)
```


## Data transformation

If your data is in the long format with a column called `Time` set to 1, 2 or 3 depending on the measurement point, then the `long_to_wide.R` script can transform the data into the correct format. The script assumes that only the selected questions are still in the dataset, with the same order of categories compared to the original data. We have already saved the dataframe as an Rdata file, which is easy to load. To import data from a csv file, you need to remove the # and add one before the `load` line.

```{r}
#data <- read.csv("sim2.csv", header=TRUE, sep = ",", dec = ".", row.names = NULL)
load("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/2. Data/allData.Rdata")
data = AllData
rm(AllData)
```

Now we transform the data.

```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/6. R scripts/CLPM model selection")
source("long_to_wide.R")
```

## Model configuration

Next up is the configuration of the four models, this is loading the code that specifies the specific relations between the variables in terms of definitions, regressions and variations. Currently, the four models are still the standard models, without random intercepts, that will be changed for the final version.

```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/6. R scripts/CLPM model selection")
source("RICLPM_step1.R")
source("RICLPM_step2.R")
source("RICLPM_step3.R")
source("RICLPM_step4.R")
source("regressions.R")
source("regressions4.R")
```

After configuring the models, we fit them and immediately save the models to a file, so we can restore and compare them later. We do this, because with this dataset of only 635 respondents, the models take an hour or more to fit and this time will only increase with larger datasets. It is recommended to use a computer dedicated to statistical calculations provided by the university for the full study, if the dataset is a lot larger than the current one.

## Configural model
The configural model is the baseline model that assumes slight common sense restrictions on the error structure, to reduce the amount of parameters that need to be estimated. The main one here is to set the covariance structure to equality between waves. The other thing this model tests for is form-invariance, or that the structure of the model, the relation between variables and factors, is the same across waves. There is nothing to compare this model to, so all that is really important is that certain fit measures, such as CFI, are high enough.
```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/8. Fitted models")
configural.fit <- cfa(paste(riclpm.1, regressions), data = data[1:200, -1], 
                  estimator = "MLR", se = "robust", 
                  missing = "ML",std.lv = TRUE, optim.dx.tol=0.01)
save(configural.fit, file = "fit_config_sampleAll.Rdata")
```


## Weak measurement invariance
Next we run the model that checks for weak measurement invariance, or metric invariance. What this means is that we fit the model with the constraints that the factor loadings, or the way we calculate the factors, are invariant at different times. It is vital that this model doesn't fit worse than the configural model, because otherwise we do not have measurement invariance and the meaning of a factor at time 1 and time 2 is different. This means thet we cannot run a regression model over time.
```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/8. Fitted models")
weak.fit <- cfa(paste(riclpm.2, regressions), data = data[1:200, -1], 
                  estimator = "MLR", se = "robust", 
                  missing = "ML",std.lv = TRUE, optim.dx.tol=0.01)
save(weak.fit, file = "fit_weak_sampleAll.Rdata")
```


## Strong measurement invariance
The third model checks for strong measurement invariance. If there is evidence for weak measurement invariance, then we can move on to seeing if strong measurement invariance, or scalar invariance, is present. Strong measurement invariance does not only hold that the factor loadings are similar at different times, but their intercepts, or baselines are also the same at different times. Another way to look at this, is that the averages do not change across waves. 
```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/8. Fitted models")
strong.fit <- cfa(paste(riclpm.3, regressions), data = data[1:200, -1], 
                  estimator = "MLR", se = "robust", 
                  missing = "ML",std.lv = TRUE, optim.dx.tol=0.01)
save(strong.fit, file = "fit_strong_sampleAll.Rdata")
```


## Strict measurement invariance
This is the strongest type of measurement invariance, also the latent Random Intercept model. This model requires that the random intercepts are at the latent variable level, ensuring that the between subject error are the same for all indicators (measurements). This is a very strict requirement and generally not the case, since the idea behind error terms is that they capture some variation that is not accounted for and it is unusual that this is a static value.
```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/8. Fitted models")
latent.fit <- cfa(paste(riclpm.4, regressions4), data = data[1:200, -1], 
                  estimator = "MLR", se = "robust", 
                  missing = "ML",std.lv = TRUE, optim.dx.tol=0.01)
save(latent.fit, file = "fit_latent_sampleAll.Rdata")
```


## Comparing fit measures

First we load the fitted models, if they are not still in the environment after running the models.

```{r}
setwd("C:/Users/hackmannt/OneDrive - Universiteit Leiden/Documents OneDrive/T Studie/Statistical Science/Statistical Consulting/8. Fitted models")
load("fit_config_sampleSim.Rdata")
load("fit_metric_sampleSim.Rdata")
load("fit_scalar_sampleSim.Rdata")
load("fit_residual_sampleSim.Rdata")
``` 

Next we generate the table comparing all the fitted values of the models and can inspect this.
```{r}
model.comparison = round( cbind ( configural = inspect( configural.fit, 'fit.measures' ), 
               weak = inspect( metric.fit, 'fit.measures' ), 
               strong = inspect( scalar.fit, 'fit.measures' ),
               latent = inspect( residual.fit, 'fit.measures') ), 3 )
print(model.comparison)
```

As we said earlier, we need to check for configural invariance by looking at some of the fit measures, most notably, the CFI we ideally want above 0.95, but definitely above 0.90 at the lowest and the Root Mean Squared Error of Approximation (RMSEA) and the Standardized Root Mean Square Residual (SRMR) below 0.08. We will check for these values for the configural model to see if the factor model fits the data at all. We use the robust fit metrics for these, since they are less prone to variances.

```{r}
conf_fit = c(model.comparison[c("cfi.robust", "rmsea.robust", "srmr"), 1])
if(conf_fit[1] < 0.90){
  print(paste("The model does not fit the data, given the robust CFI value of", conf_fit[1], ", which is lower than 0.90.") )
}

if(conf_fit[1] < 0.95 & conf_fit[1] > 0.90){
  print(paste("The model fits the data, given the robust CFI value of", conf_fit[1], ", but not that well, given the value is below 0.95.") )
}

if(conf_fit[2] > 0.08){
  print(paste("The model does not fit the data, given the robust RMSEA value of", conf_fit[2], ", which is greater than 0.08.") )
}

if(conf_fit[3] > 0.08){
  print(paste("The model does not fit the data, given the SRMR value of", conf_fit[3], ", which is greater than 0.08.") )
}

if(conf_fit[1] > 0.90 & conf_fit[2] < 0.08 & conf_fit[3] < 0.08 ){
  print(paste("The model fits the data according to all three important metrics.") )
}
```

When our configural model fits the data, we need to decide between the four different levels of measurement invariance to choose the best model. Often, it is better to choose the more parsimonous model, the one with the fewest parameters that need estimation. This helps with interpretation, since there are fewer things to take into account. We use 4 metrics to compare the models with. The first is the likelihood ratio test statistic, which is a statistic that compares nested models using a $\chi^2$ test statistic. If that test results in a p-value greater than 0.05, we can say that there is no observable difference between the two models, and we can choose the more parsimonous model. The second statistic is the $\Delta CFI$, which is the difference in the robust CFI value between the two models. If the $\Delta CFI$ is higher than $-0.01$ (the value is always negative), we can choose the more parsimonous model, since the difference isn't that large. Finally we look at the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). These two measures are very similar, but the BIC tends to prefer more parsimonous models. Both the AIC and BIC indicate the preferred model from the four by the lowest score. Between these four metrics, we choose the model that is preferred by the majority of them. In case of a tie, we choose the one with the lowest BIC value.

```{r}
d_chisq = model.comparison["chisq.scaled", 2:4]-model.comparison["chisq.scaled", 1:3]
df = model.comparison["df.scaled", ]
d_df = df[2:4]-df[1:3]
scale = model.comparison["chisq.scaling.factor", ]
cd = (df[2:4]*scale[2:4]-df[1:3]*scale[1:3])/d_df
chisq.T = d_chisq/cd
p = dchisq(chisq.T, df=d_df)

fit.decisions <- rbind(model.comparison["npar", ], c('-',d_chisq), c('-', p), model.comparison["cfi.robust",],
                       c('-', model.comparison["cfi.robust",2:4]-model.comparison["cfi.robust",1:3]),
                       c('-',model.comparison["bic", 2:4]-model.comparison["bic", 1:3]),
                       c('-', model.comparison["aic", 2:4]-model.comparison["aic", 1:3])
                       )
colnames(fit.decisions) = c("Configural", "Weak MI", "Strong MI", "Latent RI")
rownames(fit.decisions) = c("#Parameters", "\U0394\U03C7\U00B2", "p-value", "Robust CFI", "\U0394 CFI", "\U0394 BIC", "\U0394 AIC")

rm(d_chisq, df, d_df, scale, cd, chisq.T, p)
fit.decisions
```

It can be hard to read from the table what is the preferred model, so we run the following script to choose for us:

```{r}
chisq_options = which( as.double(fit.decisions[3, 2:4]) > 0.05 )
if (length(chisq_options) > 0){
  chisq_choice = chisq_options[length(chisq_options)]
} else {
  chisq_choice = 1
}
cfi_options = which( as.double(fit.decisions[5, 2:4]) < -0.01 )
if (length(cfi_options) > 0){
  cfi_choice = cfi_options[1]
} else {
  cfi_choice = 4
}
aic_options = which( as.double(fit.decisions[7, 2:4]) > 0 )
if (length(aic_options) > 0){
  aic_choice = aic_options[1]
} else {
  aic_choice = 4
}
bic_options = which( as.double(fit.decisions[6, 2:4]) > 0 )
if (length(bic_options) > 0){
  bic_choice = bic_options[1]
} else {
  bic_choice = 4
}
choices = rbind(chisq_choice, cfi_choice, aic_choice, bic_choice)
colnames(choices) = "Index"
rownames(choices) = c("ChiSq test", "CFI", "AIC", "BIC")
choices

counts = c(sum(choices == 1), sum(choices == 2), sum(choices == 3), sum(choices == 4))
if (length(max(counts))==1){
  index = which.max(counts)
  print(paste("The model of choice according to the largest number of fit metrics is the", colnames(fit.decisions)[index], "model"))
} else {
  index = counts[4]
  print(paste("Since the fit measures are tied, we choose the one which is best according to the BIC for most parsimony, 
              which is the ", colnames(fit.decisions)[index], " model"))
}
rm(chisq_options, chisq_choice, cfi_options, cfi_choice, aic_options, aic_choice, bic_options, bic_choice)
```

So we see which model we need to use. In this case we run the fit again, now with the added regressions and then we can check the regression parameters. Using this ordering, ensures that we cannot make the choice of model based on our preferred outcome. We choose the correct fit based on pre-set measures and then the outcome is decided for us.

Check once again that it conforms the fit measures of the robust CFI, robust RMSEA and SRMR.

```{r}
c(model.comparison[c("cfi.robust", "rmsea.robust", "srmr"), which.max(counts)])
```